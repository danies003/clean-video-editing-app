"""
LLM-powered video editor that interprets analysis results and generates intelligent editing plans.
"""

import json
import logging
import random
import os
import time
from typing import Dict, List, Optional, Any
from dataclasses import dataclass
from enum import Enum

import google.generativeai as genai

from app.models.schemas import VideoAnalysisResult, EditStyle, MoviePyRenderingPlan, MoviePySegment, MoviePyEffect, MoviePyTransition

logger = logging.getLogger(__name__)


class LLMProvider(Enum):
    """Supported LLM providers"""
    MOCK = "mock"  # For testing without real LLM
    OPENAI = "openai"
    ANTHROPIC = "anthropic"
    LOCAL = "local"
    GEMINI = "gemini"


@dataclass
class EditingDecision:
    """A single editing decision"""
    start_time: float
    end_time: float
    effect: str
    intensity: float
    reasoning: str


@dataclass
class EditingPlan:
    """Complete editing plan generated by LLM"""
    style: str
    target_duration: float
    segments: List[EditingDecision]
    transitions: List[str]
    effects: List[str]
    reasoning: str
    confidence: float


class LLMEditor:
    """LLM-powered video editor that generates intelligent editing plans"""
    
    def __init__(self, provider: LLMProvider = LLMProvider.GEMINI):
        """Initialize the LLM editor with specified provider."""
        self.provider = provider
        self.logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")
        
        # Initialize Gemini AI
        if self.provider == LLMProvider.GEMINI:
            gemini_api_key = os.environ.get("GEMINI_API_KEY")
            if gemini_api_key:
                genai.configure(api_key=gemini_api_key)
                self.logger.info("âœ… [LLM_EDITOR] Gemini AI initialized")
            else:
                self.logger.error("âŒ [LLM_EDITOR] GEMINI_API_KEY not found")
                raise ValueError("GEMINI_API_KEY environment variable is required for Gemini provider")
        
        # Dynamically load effects from enhanced shader library
        self.available_effects = self._load_available_effects()
    
    def _load_available_effects(self) -> Dict[str, List[str]]:
        """Dynamically load available effects from the enhanced shader library."""
        try:
            from app.editor.enhanced_shader_library import EnhancedShaderLibrary, EffectType
            
            # Get all available effects from the enhanced shader library
            shader_library = EnhancedShaderLibrary()
            all_effects = shader_library.get_available_effects()
            
            # Categorize effects based on their type
            categorized_effects = {
                "audio_reactive": [],
                "motion_based": [],
                "style_transfer": [],
                "geometric": [],
                "color_effects": [],
                "transitions": []
            }
            
            # Map effects to categories based on EffectType enum
            for effect in all_effects:
                if effect in ["beat_sync", "frequency_glow", "audio_spectrum", "audio_pulse", "bass_boost", "treble_enhance", "rhythm_visualizer", "sound_wave"]:
                    categorized_effects["audio_reactive"].append(effect)
                elif effect in ["motion_blur", "scene_transition", "camera_shake", "zoom_pan", "parallax", "stabilization", "motion_trail", "velocity_blur"]:
                    categorized_effects["motion_based"].append(effect)
                elif effect in ["cinematic", "vintage", "film_grain", "retro_80s", "neon_cyberpunk", "black_white", "sepia_tone", "hdr_enhance"]:
                    categorized_effects["style_transfer"].append(effect)
                elif effect in ["fisheye", "twirl", "perspective", "barrel_distortion", "pincushion", "spiral", "kaleidoscope", "fractal_zoom"]:
                    categorized_effects["geometric"].append(effect)
                elif effect in ["color_grading", "high_contrast", "saturation_boost", "hue_shift", "color_invert", "selective_color", "split_toning", "color_bleed"]:
                    categorized_effects["color_effects"].append(effect)
                elif effect in ["cross_dissolve", "slide", "zoom", "fade", "wipe", "spiral_transition", "pixelate", "glitch_transition"]:
                    categorized_effects["transitions"].append(effect)
            
            self.logger.info(f"âœ… [LLM_EDITOR] Dynamically loaded {len(all_effects)} effects from enhanced shader library")
            return categorized_effects
            
        except ImportError as e:
            self.logger.warning(f"âš ï¸ [LLM_EDITOR] Could not import enhanced shader library: {e}")
            # Fallback to basic effects if enhanced library is not available
            return {
                "audio_reactive": ["beat_sync", "audio_pulse"],
                "motion_based": ["motion_blur", "scene_transition"],
                "style_transfer": ["cinematic", "vintage"],
                "geometric": ["twirl", "perspective"],
                "color_effects": ["color_grading", "high_contrast"],
                "transitions": ["cross_dissolve", "slide", "zoom"]
            }
    
    def _get_dynamic_available_effects(self) -> Dict[str, List[str]]:
        """Dynamically get available effects from the shader library."""
        try:
            from app.editor.enhanced_shader_library import EnhancedShaderLibrary
            shader_library = EnhancedShaderLibrary()
            available_effects = shader_library.get_available_effects()
            
            # Categorize effects based on their names
            categorized_effects = {
                "Visual Effects": [],
                "Audio Reactive Effects": [],
                "Motion Based Effects": [],
                "Style Transfer Effects": [],
                "Geometric Effects": [],
                "Color Effects": [],
                "Speed Effects": [],
                "Transition Effects": []
            }
            
            # Define effect categories based on naming patterns
            for effect in available_effects:
                if effect in ["beat_sync", "frequency_visualizer", "volume_wave", "audio_pulse"]:
                    categorized_effects["Audio Reactive Effects"].append(effect)
                elif effect in ["motion_blur", "optical_flow", "scene_transition", "motion_trail"]:
                    categorized_effects["Motion Based Effects"].append(effect)
                elif effect in ["cinematic", "vintage", "cyberpunk", "film_noir", "cartoon"]:
                    categorized_effects["Style Transfer Effects"].append(effect)
                elif effect in ["fisheye", "twirl", "warp", "perspective"]:
                    categorized_effects["Geometric Effects"].append(effect)
                elif effect in ["color_grading", "duotone", "invert", "high_contrast"]:
                    categorized_effects["Color Effects"].append(effect)
                elif effect in ["speed_up", "slow_motion", "speed"]:
                    categorized_effects["Speed Effects"].append(effect)
                elif effect in ["cross_dissolve", "slide", "zoom", "whip_pan", "spin", "glitch"]:
                    categorized_effects["Transition Effects"].append(effect)
                else:
                    categorized_effects["Visual Effects"].append(effect)
            
            return categorized_effects
            
        except Exception as e:
            logger.warning(f"Could not load dynamic effects from shader library: {e}")
            # Fallback to hardcoded effects
            return {
                "Visual Effects": ["cinematic", "vintage", "cyberpunk", "film_noir", "cartoon", "glitch", "color_grading", "duotone", "invert", "high_contrast"],
                "Audio Reactive Effects": ["beat_sync", "frequency_visualizer", "volume_wave", "audio_pulse"],
                "Motion Based Effects": ["motion_blur", "optical_flow", "scene_transition", "motion_trail"],
                "Style Transfer Effects": ["cinematic", "vintage", "cyberpunk", "film_noir", "cartoon"],
                "Geometric Effects": ["fisheye", "twirl", "warp", "perspective"],
                "Color Effects": ["color_grading", "duotone", "invert", "high_contrast"],
                "Speed Effects": ["speed_up", "slow_motion", "speed"],
                "Transition Effects": ["cross_dissolve", "slide", "zoom", "whip_pan", "spin", "glitch"]
            }

    def _build_dynamic_effects_prompt(self) -> str:
        """Build dynamic effects prompt from shader library."""
        categorized_effects = self._get_dynamic_available_effects()
        
        prompt_sections = []
        
        for category, effects in categorized_effects.items():
            if effects:  # Only include categories that have effects
                effects_list = "\n".join([f"- {effect}" for effect in effects])
                prompt_sections.append(f"{category}:\n{effects_list}")
        
        return "\n\n".join(prompt_sections)
    
    def generate_editing_plan(
        self, 
        analysis_result: VideoAnalysisResult, 
        style: EditStyle,
        target_duration: Optional[float] = None
    ) -> EditingPlan:
        """
        Generate an intelligent editing plan based on analysis results.
        
        Args:
            analysis_result: Video analysis results
            style: Editing style (tiktok, youtube, cinematic)
            target_duration: Target duration in seconds (optional)
            
        Returns:
            EditingPlan with intelligent editing decisions
        """
        self.logger.info(f"Generating {style.value} editing plan for video")
        
        # Generate prompt based on style and analysis
        prompt = self._build_prompt(analysis_result, style, target_duration)
        
        # Get LLM response
        llm_response = self._call_llm(prompt)
        
        # Parse response into editing plan
        editing_plan = self._parse_llm_response(llm_response, style, analysis_result)
        
        self.logger.info(f"Generated editing plan with {len(editing_plan.segments)} segments")
        return editing_plan
    
    def generate_moviepy_rendering_plan(
        self,
        analysis_result: VideoAnalysisResult,
        style: EditStyle,
        source_video_path: str,
        output_path: str,
        target_duration: Optional[float] = None,
        output_format: str = "mp4",
        output_quality: str = "high"
    ) -> MoviePyRenderingPlan:
        """
        Generate a complete MoviePy-compatible rendering plan based on analysis results.
        
        Args:
            analysis_result: Video analysis results
            style: Editing style (tiktok, youtube, cinematic)
            source_video_path: Path to source video file
            output_path: Path for output video file
            target_duration: Target duration in seconds (optional)
            output_format: Output video format
            output_quality: Output quality preset
            
        Returns:
            MoviePyRenderingPlan with complete rendering instructions
        """
        self.logger.info(f"Generating {style.value} MoviePy rendering plan for video")
        
        # Generate comprehensive prompt for MoviePy rendering
        prompt = self._build_moviepy_prompt(analysis_result, style, target_duration)
        
        # Get LLM response
        llm_response = self._call_llm(prompt)
        
        # Parse response into MoviePy rendering plan
        rendering_plan = self._parse_moviepy_response(
            llm_response, style, analysis_result, source_video_path, 
            output_path, target_duration, output_format, output_quality
        )
        
        self.logger.info(f"Generated MoviePy rendering plan with {len(rendering_plan.segments)} segments")
        return rendering_plan
    
    def _build_prompt(self, analysis: VideoAnalysisResult, style: EditStyle, target_duration: Optional[float]) -> str:
        """Build LLM prompt based on analysis results and style"""
        
        # Extract key analysis data
        duration = analysis.duration
        fps = analysis.fps
        resolution = analysis.resolution
        
        # Beat detection data
        beat_timestamps = analysis.beat_detection.timestamps if analysis.beat_detection else []
        bpm = analysis.beat_detection.bpm if analysis.beat_detection else 0
        
        # Motion analysis data
        motion_spikes = analysis.motion_analysis.motion_spikes if analysis.motion_analysis else []
        scene_changes = analysis.motion_analysis.scene_changes if analysis.motion_analysis else []
        
        # Audio analysis data
        volume_levels = analysis.audio_analysis.volume_levels if analysis.audio_analysis else []
        silence_periods = analysis.audio_analysis.silence_periods if analysis.audio_analysis else []
        
        # Advanced effects and color grading capabilities
        effects_capabilities = f"""
AVAILABLE ADVANCED EFFECTS AND COLOR GRADING:

{self._build_dynamic_effects_prompt()}

Color Grading Styles:
- cinematic: High contrast, warm tones, film-like look
- vintage: Retro film grain, warm temperature, desaturated
- cold: Blue tones, cool temperature, desaturated
- warm: Orange/red tones, warm temperature, enhanced saturation
- high_contrast: Dramatic shadows and highlights
- low_contrast: Soft, flat appearance
- saturated: Vibrant, colorful look
- desaturated: Muted, subtle colors
- dramatic: High contrast with moody tones
- romantic: Soft, warm, dreamy look
- action: High saturation, dynamic contrast
- moody: Dark, atmospheric tones

Advanced Transitions:
- whip_pan: Smooth horizontal pan with motion blur
- zoom_blur: Zoom with blur effects
- shake: Controlled camera shake
- match_cut: Content-aware transition
- crossfade: Smooth fade between clips
- dissolve: Gradual blend transition
- slide: Sliding motion transition
- fade: Simple fade in/out
- cut: Direct cut
- zoom_transition: Zoom-based transition
- morph: Morphing transition
- glitch_transition: Digital glitch effect

Speed Effects:
- speed_up: Increase playback speed (1.2x-2.0x)
- slow_motion: Decrease playback speed (0.3x-0.8x)
- normal: Standard playback speed
- variable_speed: Dynamic speed changes
- freeze_frame: Pause on specific frame

Text and Caption Effects:
- super_text: Custom font rendering with animations
- mood_templates: Contextual text overlays
- caption_overlays: Dynamic caption positioning
- text_animations: Slide-in, fade, zoom text effects

Split Screen Effects:
- vertical_split: 2x1 vertical layout
- horizontal_split: 1x2 horizontal layout
- quad_split: 2x2 grid layout
- diagonal_split: Diagonal arrangement
- custom_split: User-defined layouts

Background Music Integration:
- beat_sync: Sync effects to music beats
- tempo_matching: Match video pace to music tempo
- audio_reactive: Visual effects respond to audio
- music_transitions: Smooth music transitions
"""
        
        # Style-specific instructions with advanced effects
        style_instructions = {
            EditStyle.TIKTOK: f"""
TikTok Style Guidelines:
- Fast-paced, attention-grabbing cuts
- Use beat-synchronized cuts when beats are detected
- Emphasize high-motion moments
- Keep segments short (0.5-2 seconds)
- Use dynamic transitions (whip_pan, zoom_blur, shake)
- Apply vibrant color grading (saturated, color_pop, dramatic)
- Use visual effects for impact (glitch, lens_flare, vignette)
- Target duration: 15-60 seconds

Recommended Effects for TikTok:
- Color Grading: saturated, dramatic, color_pop
- Visual Effects: glitch, lens_flare, vignette
- Transitions: whip_pan, zoom_blur, shake, glitch_transition
- Speed: speed_up, variable_speed
""",
            EditStyle.YOUTUBE: f"""
YouTube Style Guidelines:
- Balanced pacing with clear narrative flow
- Use scene changes for natural breaks
- Emphasize important moments
- Mix fast and slow segments
- Use smooth transitions (crossfade, dissolve, match_cut)
- Apply professional color grading (cinematic, warm, high_contrast)
- Use subtle visual effects (film_grain, vignette)
- Target duration: 2-10 minutes

Recommended Effects for YouTube:
- Color Grading: cinematic, warm, high_contrast
- Visual Effects: film_grain, vignette, dramatic_lighting
- Transitions: crossfade, dissolve, match_cut, slide
- Speed: normal, variable_speed, slow_motion
""",
            EditStyle.CINEMATIC: f"""
Cinematic Style Guidelines:
- Slow, deliberate pacing
- Emphasize dramatic moments
- Use long takes for emotional impact
- Smooth, elegant transitions
- Focus on visual storytelling
- Apply cinematic color grading (cinematic, moody, dramatic)
- Use atmospheric visual effects (vintage_film, bleach_bypass)
- Target duration: 3-15 minutes

Recommended Effects for Cinematic:
- Color Grading: cinematic, moody, dramatic, vintage
- Visual Effects: vintage_film, bleach_bypass, film_grain, vignette
- Transitions: crossfade, dissolve, fade, morph
- Speed: slow_motion, normal, freeze_frame
"""
        }
        
        prompt = f"""
You are an expert video editor with access to advanced effects and color grading capabilities. Analyze the following video data and create an intelligent editing plan using the available effects.

VIDEO ANALYSIS DATA:
- Duration: {duration:.2f} seconds
- FPS: {fps}
- Resolution: {resolution[0]}x{resolution[1]}
- Target Duration: {target_duration if target_duration else 'Keep original'}

BEAT DETECTION:
- BPM: {bpm}
- Beat timestamps: {beat_timestamps[:10] if len(beat_timestamps) > 10 else beat_timestamps}

MOTION ANALYSIS:
- Motion spikes: {motion_spikes[:10] if len(motion_spikes) > 10 else motion_spikes}
- Scene changes: {scene_changes[:10] if len(scene_changes) > 10 else scene_changes}

AUDIO ANALYSIS:
- Volume levels: {len(volume_levels)} data points
- Silence periods: {len(silence_periods)} periods

{effects_capabilities}

{style_instructions.get(style, '')}

TASK: Create an intelligent editing plan that uses the available advanced effects and color grading to enhance the video according to the style guidelines. Consider the analysis data to make informed decisions about timing, effects, and transitions.

Return ONLY a valid JSON object with this exact structure:
{{
    "style": "string",
    "target_duration": float,
    "segments": [
        {{
            "start_time": float,
            "end_time": float,
            "effect": "string (from available effects list)",
            "intensity": float (0.1 to 2.0),
            "reasoning": "string"
        }}
    ],
    "transitions": ["string (from available transitions list)"],
    "effects": ["string (from available effects list)"],
    "reasoning": "string",
    "confidence": float (0.0 to 1.0)
}}
"""
        return prompt
    
    def _call_llm(self, prompt: str) -> str:
        """Call the LLM with the given prompt"""
        
        if self.provider == LLMProvider.MOCK:
            return self._mock_llm_call(prompt)
        elif self.provider == LLMProvider.OPENAI:
            return self._call_openai(prompt)
        elif self.provider == LLMProvider.ANTHROPIC:
            return self._call_anthropic(prompt)
        elif self.provider == LLMProvider.GEMINI:
            return self._call_gemini(prompt)
        else:
            raise ValueError(f"Unsupported LLM provider: {self.provider}")
    
    def _mock_llm_call(self, prompt: str) -> str:
        """Mock LLM call for testing"""
        self.logger.info("Using mock LLM response")
        
        # Parse the prompt to extract key information
        if "tiktok" in prompt.lower():
            return self._generate_mock_tiktok_response()
        elif "youtube" in prompt.lower():
            return self._generate_mock_youtube_response()
        elif "cinematic" in prompt.lower():
            return self._generate_mock_cinematic_response()
        else:
            return self._generate_mock_default_response()
    
    def _generate_mock_tiktok_response(self) -> str:
        """Generate mock TikTok-style editing plan"""
        return json.dumps({
            "style": "tiktok",
            "target_duration": 30.0,
            "segments": [
                {
                    "start_time": 0.0,
                    "end_time": 1.5,
                    "effect": "speed_up",
                    "intensity": 1.2,
                    "reasoning": "Fast opening to grab attention"
                },
                {
                    "start_time": 1.5,
                    "end_time": 3.0,
                    "effect": "normal",
                    "intensity": 1.0,
                    "reasoning": "Show key moment at normal speed"
                },
                {
                    "start_time": 3.0,
                    "end_time": 4.5,
                    "effect": "zoom",
                    "intensity": 1.3,
                    "reasoning": "Dramatic zoom effect on action"
                },
                {
                    "start_time": 4.5,
                    "end_time": 6.0,
                    "effect": "slow_motion",
                    "intensity": 0.7,
                    "reasoning": "Slow motion for impact"
                }
            ],
            "transitions": ["cut", "slide", "fade"],
            "effects": ["speed_up", "slow_motion", "zoom"],
            "reasoning": "TikTok-style fast-paced editing with attention-grabbing effects and beat-synchronized cuts",
            "confidence": 0.88
        })
    
    def _generate_mock_youtube_response(self) -> str:
        """Generate mock YouTube-style editing plan"""
        return json.dumps({
            "style": "youtube",
            "target_duration": 120.0,
            "segments": [
                {
                    "start_time": 0.0,
                    "end_time": 5.0,
                    "effect": "normal",
                    "intensity": 1.0,
                    "reasoning": "Introduction with clear setup"
                },
                {
                    "start_time": 5.0,
                    "end_time": 15.0,
                    "effect": "speed_up",
                    "intensity": 1.1,
                    "reasoning": "Build momentum for main content"
                },
                {
                    "start_time": 15.0,
                    "end_time": 25.0,
                    "effect": "normal",
                    "intensity": 1.0,
                    "reasoning": "Key content at normal pace"
                }
            ],
            "transitions": ["crossfade", "dissolve", "cut"],
            "effects": ["speed_up", "color_grade"],
            "reasoning": "YouTube-style balanced editing with clear narrative flow and professional transitions",
            "confidence": 0.92
        })
    
    def _generate_mock_cinematic_response(self) -> str:
        """Generate mock cinematic-style editing plan"""
        return json.dumps({
            "style": "cinematic",
            "target_duration": 180.0,
            "segments": [
                {
                    "start_time": 0.0,
                    "end_time": 10.0,
                    "effect": "slow_motion",
                    "intensity": 0.8,
                    "reasoning": "Cinematic opening with dramatic pacing"
                },
                {
                    "start_time": 10.0,
                    "end_time": 25.0,
                    "effect": "normal",
                    "intensity": 1.0,
                    "reasoning": "Establish emotional connection"
                },
                {
                    "start_time": 25.0,
                    "end_time": 35.0,
                    "effect": "color_grade",
                    "intensity": 1.2,
                    "reasoning": "Enhanced visual storytelling"
                }
            ],
            "transitions": ["fade", "dissolve", "crossfade"],
            "effects": ["slow_motion", "color_grade", "blur"],
            "reasoning": "Cinematic-style editing with deliberate pacing and emotional storytelling",
            "confidence": 0.95
        })
    
    def _generate_mock_default_response(self) -> str:
        """Generate default mock response"""
        return json.dumps({
            "style": "default",
            "target_duration": 60.0,
            "segments": [
                {
                    "start_time": 0.0,
                    "end_time": 5.0,
                    "effect": "normal",
                    "intensity": 1.0,
                    "reasoning": "Standard opening"
                }
            ],
            "transitions": ["cut", "fade"],
            "effects": ["normal"],
            "reasoning": "Standard editing approach",
            "confidence": 0.75
        })
    
    def _call_openai(self, prompt: str) -> str:
        """Call OpenAI API with the provided prompt"""
        try:
            import openai
            
            # Get API key from environment or use the provided one
            api_key = os.getenv("OPENAI_API_KEY")
            
            self.logger.info(f"ğŸ” [OPENAI_DEBUG] API Key check - Length: {len(api_key) if api_key else 0}")
            self.logger.info(f"ğŸ” [OPENAI_DEBUG] API Key starts with: {api_key[:20] if api_key else 'None'}...")
            
            if not api_key:
                self.logger.error("âŒ [OPENAI_DEBUG] OpenAI API key not found")
                return self._mock_llm_call(prompt)
            
            # Initialize OpenAI client
            self.logger.info("ğŸ” [OPENAI_DEBUG] Initializing OpenAI client...")
            client = openai.OpenAI(api_key=api_key)
            self.logger.info("âœ… [OPENAI_DEBUG] OpenAI client initialized successfully")
            
            # Log prompt details
            self.logger.info(f"ğŸ” [OPENAI_DEBUG] Prompt length: {len(prompt)} characters")
            self.logger.info(f"ğŸ” [OPENAI_DEBUG] Prompt preview: {prompt[:200]}...")
            
            self.logger.info("ğŸš€ [OPENAI_DEBUG] Making OpenAI API call...")
            
            # Prepare API call parameters
            api_params = {
                "model": "gpt-4-turbo-preview",
                "messages": [
                    {
                        "role": "system",
                        "content": "You are an expert video editor with deep knowledge of editing techniques, pacing, and storytelling. You analyze video data and create intelligent editing plans that enhance the viewer experience. You MUST return ONLY valid JSON with the exact structure specified. Do not include any additional text, explanations, or formatting outside the JSON object."
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                "temperature": 0.7,
                "max_tokens": 2000,
                "response_format": {"type": "json_object"}
            }
            
            self.logger.info(f"ğŸ” [OPENAI_DEBUG] API Parameters: {api_params}")
            
            # Make API call with detailed logging
            try:
                self.logger.info("ğŸ“¡ [OPENAI_DEBUG] Sending request to OpenAI API...")
                response = client.chat.completions.create(**api_params)
                self.logger.info("âœ… [OPENAI_DEBUG] OpenAI API call completed successfully")
                
                # Log response details
                self.logger.info(f"ğŸ” [OPENAI_DEBUG] Response object type: {type(response)}")
                self.logger.info(f"ğŸ” [OPENAI_DEBUG] Response has choices: {hasattr(response, 'choices')}")
                self.logger.info(f"ğŸ” [OPENAI_DEBUG] Number of choices: {len(response.choices) if hasattr(response, 'choices') else 'N/A'}")
                
                if hasattr(response, 'choices') and len(response.choices) > 0:
                    choice = response.choices[0]
                    self.logger.info(f"ğŸ” [OPENAI_DEBUG] First choice type: {type(choice)}")
                    self.logger.info(f"ğŸ” [OPENAI_DEBUG] Choice has message: {hasattr(choice, 'message')}")
                    
                    if hasattr(choice, 'message'):
                        message = choice.message
                        self.logger.info(f"ğŸ” [OPENAI_DEBUG] Message type: {type(message)}")
                        self.logger.info(f"ğŸ” [OPENAI_DEBUG] Message has content: {hasattr(message, 'content')}")
                        
                        if hasattr(message, 'content'):
                            llm_response = message.content
                            self.logger.info(f"ğŸ” [OPENAI_DEBUG] Response content length: {len(llm_response) if llm_response else 0}")
                            self.logger.info(f"ğŸ” [OPENAI_DEBUG] Response content preview: {llm_response[:200] if llm_response else 'None'}...")
                            
                            if not llm_response:
                                self.logger.error("âŒ [OPENAI_DEBUG] OpenAI API returned empty response content")
                                return self._mock_llm_call(prompt)
                            
                            # Log the full JSON response
                            self.logger.info("ğŸ“„ [OPENAI_DEBUG] Full JSON Response:")
                            self.logger.info("=" * 80)
                            self.logger.info(llm_response)
                            self.logger.info("=" * 80)
                            
                            self.logger.info("âœ… [OPENAI_DEBUG] Successfully extracted response content")
                            return llm_response
                        else:
                            self.logger.error("âŒ [OPENAI_DEBUG] Message object has no content attribute")
                            return self._mock_llm_call(prompt)
                    else:
                        self.logger.error("âŒ [OPENAI_DEBUG] Choice object has no message attribute")
                        return self._mock_llm_call(prompt)
                else:
                    self.logger.error("âŒ [OPENAI_DEBUG] Response has no choices or empty choices")
                    return self._mock_llm_call(prompt)
                
            except Exception as api_error:
                self.logger.error(f"âŒ [OPENAI_DEBUG] OpenAI API call failed with error: {api_error}")
                self.logger.error(f"âŒ [OPENAI_DEBUG] Error type: {type(api_error)}")
                self.logger.error(f"âŒ [OPENAI_DEBUG] Error details: {str(api_error)}")
                
                # Log additional error information
                self.logger.error(f"âŒ [OPENAI_DEBUG] Error attributes: {dir(api_error)}")
                self.logger.error(f"âŒ [OPENAI_DEBUG] Error string representation: {str(api_error)}")
                
                return self._mock_llm_call(prompt)
            
        except ImportError as import_error:
            self.logger.error(f"âŒ [OPENAI_DEBUG] OpenAI package import failed: {import_error}")
            self.logger.error("âŒ [OPENAI_DEBUG] Install with: pip install openai")
            return self._mock_llm_call(prompt)
        except Exception as e:
            self.logger.error(f"âŒ [OPENAI_DEBUG] Unexpected error in OpenAI call: {e}")
            self.logger.error(f"âŒ [OPENAI_DEBUG] Error type: {type(e)}")
            import traceback
            self.logger.error(f"âŒ [OPENAI_DEBUG] Full traceback: {traceback.format_exc()}")
            self.logger.warning("ğŸ”„ [OPENAI_DEBUG] Falling back to mock response")
            return self._mock_llm_call(prompt)
    
    def _call_anthropic(self, prompt: str) -> str:
        """Call Anthropic API (placeholder for future implementation)"""
        # TODO: Implement Anthropic API call
        self.logger.warning("Anthropic API not implemented, using mock response")
        return self._mock_llm_call(prompt)
    
    def _call_gemini(self, prompt: str) -> str:
        """Call Gemini API with the provided prompt"""
        try:
            self.logger.info("ğŸ” [GEMINI_DEBUG] Making Gemini API call...")
            
            # Log prompt details
            self.logger.info(f"ğŸ” [GEMINI_DEBUG] Prompt length: {len(prompt)} characters")
            self.logger.info(f"ğŸ” [GEMINI_DEBUG] Prompt preview: {prompt[:200]}...")
            
            # Use Gemini 2.5 Flash model
            model = genai.GenerativeModel(model_name="gemini-2.5-flash")
            
            # Generate content
            response = model.generate_content(prompt)
            
            if response and response.text:
                self.logger.info("âœ… [GEMINI_DEBUG] Gemini API call successful")
                self.logger.info(f"ğŸ“„ [GEMINI_DEBUG] Response length: {len(response.text)} characters")
                return response.text
            else:
                self.logger.error("âŒ [GEMINI_DEBUG] Gemini API returned empty response")
                self.logger.error(f"âŒ [GEMINI_DEBUG] Response object: {response}")
                raise ValueError("Gemini API returned empty response")
                
        except Exception as e:
            self.logger.error(f"âŒ [GEMINI_DEBUG] Gemini API call failed: {e}")
            self.logger.error(f"âŒ [GEMINI_DEBUG] Error type: {type(e).__name__}")
            import traceback
            self.logger.error(f"âŒ [GEMINI_DEBUG] Full traceback: {traceback.format_exc()}")
            raise
    
    def _parse_llm_response(self, response: str, style: EditStyle, analysis: VideoAnalysisResult) -> EditingPlan:
        """Parse LLM response into EditingPlan object"""
        try:
            # Parse JSON response
            data = json.loads(response)
            
            # Validate required fields
            required_fields = ["style", "target_duration", "segments", "transitions", "effects", "reasoning", "confidence"]
            for field in required_fields:
                if field not in data:
                    raise ValueError(f"Missing required field: {field}")
            
            # Convert segments to EditingDecision objects
            segments = []
            for segment_data in data["segments"]:
                segment = EditingDecision(
                    start_time=float(segment_data["start_time"]),
                    end_time=float(segment_data["end_time"]),
                    effect=segment_data["effect"],
                    intensity=float(segment_data["intensity"]),
                    reasoning=segment_data["reasoning"]
                )
                segments.append(segment)
            
            # Create EditingPlan
            editing_plan = EditingPlan(
                style=data["style"],
                target_duration=float(data["target_duration"]),
                segments=segments,
                transitions=data["transitions"],
                effects=data["effects"],
                reasoning=data["reasoning"],
                confidence=float(data["confidence"])
            )
            
            self.logger.info(f"Successfully parsed LLM response into editing plan")
            return editing_plan
            
        except (json.JSONDecodeError, KeyError, ValueError) as e:
            import traceback
            tb = traceback.format_exc()
            self.logger.error(f"âŒ [LLM EDITOR] Failed to parse LLM response")
            self.logger.error(f"âŒ [LLM EDITOR] Exception type: {type(e).__name__}")
            self.logger.error(f"âŒ [LLM EDITOR] Exception message: {str(e)}")
            self.logger.error(f"âŒ [LLM EDITOR] Full traceback:")
            self.logger.error(tb)
            self.logger.error(f"âŒ [LLM EDITOR] Raw response:")
            self.logger.error(response)
            self.logger.error(f"âŒ [LLM EDITOR] Response length: {len(response) if response else 0}")
            self.logger.error(f"âŒ [LLM EDITOR] Response preview: {response[:500] if response else 'None'}...")
            # Return fallback plan
            return self._create_fallback_plan(style, analysis)
    
    def _build_moviepy_prompt(self, analysis: VideoAnalysisResult, style: EditStyle, target_duration: Optional[float]) -> str:
        """Build comprehensive LLM prompt for MoviePy rendering plan"""
        
        # Extract key analysis data
        duration = analysis.duration
        fps = analysis.fps
        resolution = analysis.resolution
        
        # Beat detection data
        beat_timestamps = analysis.beat_detection.timestamps if analysis.beat_detection else []
        bpm = analysis.beat_detection.bpm if analysis.beat_detection else 0
        
        # Motion analysis data
        motion_spikes = analysis.motion_analysis.motion_spikes if analysis.motion_analysis else []
        scene_changes = analysis.motion_analysis.scene_changes if analysis.motion_analysis else []
        
        # Audio analysis data
        volume_levels = analysis.audio_analysis.volume_levels if analysis.audio_analysis else []
        silence_periods = analysis.audio_analysis.silence_periods if analysis.audio_analysis else []
        
        # Advanced effects and color grading capabilities for MoviePy
        effects_capabilities = """
AVAILABLE ADVANCED EFFECTS AND COLOR GRADING FOR MOVIEPY:

Color Grading Styles:
- cinematic: High contrast, warm tones, film-like look
- vintage: Retro film grain, warm temperature, desaturated
- cold: Blue tones, cool temperature, desaturated
- warm: Orange/red tones, warm temperature, enhanced saturation
- high_contrast: Dramatic shadows and highlights
- low_contrast: Soft, flat appearance
- saturated: Vibrant, colorful look
- desaturated: Muted, subtle colors
- dramatic: High contrast with moody tones
- romantic: Soft, warm, dreamy look
- action: High saturation, dynamic contrast
- moody: Dark, atmospheric tones

Visual Effects:
- glitch: Digital distortion and artifacts
- vintage_film: Old film look with grain and color shifts
- cinematic_look: Professional film appearance
- dramatic_lighting: Enhanced lighting effects
- color_pop: Selective color enhancement
- monochrome: Black and white conversion
- sepia: Vintage brown tone
- cross_process: Color film processing effect
- bleach_bypass: High contrast, desaturated look
- film_grain: Add film grain texture
- vignette: Darkened edges
- lens_flare: Light flare effects

Advanced Transitions:
- whip_pan: Smooth horizontal pan with motion blur
- zoom_blur: Zoom with blur effects
- shake: Controlled camera shake
- match_cut: Content-aware transition
- crossfade: Smooth fade between clips
- dissolve: Gradual blend transition
- slide: Sliding motion transition
- fade: Simple fade in/out
- cut: Direct cut
- zoom_transition: Zoom-based transition
- morph: Morphing transition
- glitch_transition: Digital glitch effect

Speed Effects:
- speed_up: Increase playback speed (1.2x-2.0x)
- slow_motion: Decrease playback speed (0.3x-0.8x)
- normal: Standard playback speed
- variable_speed: Dynamic speed changes
- freeze_frame: Pause on specific frame

Basic MoviePy Effects:
- fade_in, fade_out, crossfade, slide_in, slide_out, zoom_in, zoom_out
- rotate, flip, mirror, crop, resize
- brightness, contrast, saturation, gamma, blur, sharpen
- color_balance, sepia, black_white, vintage
- speed_up, slow_motion, freeze_frame, loop, reverse

Audio Effects:
- audio_normalize, audio_fade_in, audio_fade_out
- audio_speed, audio_volume, audio_echo, audio_reverb
- audio_high_pass, audio_low_pass
"""

        # Style-specific instructions with advanced effects
        style_instructions = {
            EditStyle.TIKTOK: f"""
TikTok Style Guidelines:
- Fast-paced, attention-grabbing cuts with beat synchronization
- Use dynamic transitions (whip_pan, zoom_blur, shake, glitch_transition)
- Emphasize high-motion moments with speed_up effects
- Keep segments short (0.5-2 seconds)
- Use vibrant color grading (saturated, color_pop, dramatic)
- Apply visual effects for impact (glitch, lens_flare, vignette)
- Add audio effects for impact
- Target duration: 15-60 seconds

Recommended Effects for TikTok:
- Color Grading: saturated, dramatic, color_pop
- Visual Effects: glitch, lens_flare, vignette
- Transitions: whip_pan, zoom_blur, shake, glitch_transition
- Speed: speed_up, variable_speed
""",
            EditStyle.YOUTUBE: f"""
YouTube Style Guidelines:
- Balanced pacing with clear narrative flow
- Use scene changes for natural breaks
- Emphasize important moments with subtle effects
- Mix fast and slow segments
- Use smooth transitions (crossfade, dissolve, match_cut)
- Professional color grading (cinematic, warm, high_contrast)
- Apply subtle visual effects (film_grain, vignette)
- Target duration: 2-10 minutes

Recommended Effects for YouTube:
- Color Grading: cinematic, warm, high_contrast
- Visual Effects: film_grain, vignette, dramatic_lighting
- Transitions: crossfade, dissolve, match_cut, slide
- Speed: normal, variable_speed, slow_motion
""",
            EditStyle.CINEMATIC: f"""
Cinematic Style Guidelines:
- Slow, deliberate pacing with dramatic emphasis
- Use long takes for emotional impact
- Smooth, elegant transitions (fade, dissolve, morph)
- Focus on visual storytelling
- Cinematic color grading (cinematic, moody, dramatic, vintage)
- Apply atmospheric visual effects (vintage_film, bleach_bypass)
- Audio emphasis on key moments
- Target duration: 3-15 minutes

Recommended Effects for Cinematic:
- Color Grading: cinematic, moody, dramatic, vintage
- Visual Effects: vintage_film, bleach_bypass, film_grain, vignette
- Transitions: crossfade, dissolve, fade, morph
- Speed: slow_motion, normal, freeze_frame
"""
        }
        
        prompt = f"""
You are an expert video editor and MoviePy specialist with access to advanced effects and color grading capabilities. Analyze the following video data and create a comprehensive MoviePy rendering plan using the available effects.

VIDEO ANALYSIS DATA:
- Duration: {duration:.2f} seconds
- FPS: {fps}
- Resolution: {resolution[0]}x{resolution[1]}
- Target Duration: {target_duration if target_duration else 'Keep original'}

BEAT DETECTION:
- BPM: {bpm}
- Beat timestamps: {beat_timestamps[:10] if len(beat_timestamps) > 10 else beat_timestamps}

MOTION ANALYSIS:
- Motion spikes: {motion_spikes[:10] if len(motion_spikes) > 10 else motion_spikes}
- Scene changes: {scene_changes[:10] if len(scene_changes) > 10 else scene_changes}

AUDIO ANALYSIS:
- Volume levels: {len(volume_levels)} data points
- Silence periods: {len(silence_periods)} periods

{effects_capabilities}

{style_instructions.get(style, '')}

TASK:
Create a JSON MoviePy rendering plan with the following structure:
{{
    "style": "{style.value}",
    "target_duration": {target_duration if target_duration else duration},
    "confidence": 0.85,
    "reasoning": "Detailed explanation of editing strategy",
    "segments": [
        {{
            "start_time": 0.0,
            "end_time": 2.0,
            "speed": 1.0,
            "volume": 1.0,
            "rotation": 0.0,
            "brightness": 1.0,
            "contrast": 1.0,
            "saturation": 1.0,
            "gamma": 1.0,
            "blur": 0.0,
            "sharpness": 1.0,
            "effects": [
                {{
                    "effect_type": "fade_in",
                    "parameters": {{"duration": 0.5}},
                    "intensity": 1.0
                }}
            ],
            "audio_effects": [
                {{
                    "effect_type": "audio_normalize",
                    "parameters": {{}},
                    "intensity": 1.0
                }}
            ],
            "transition_in": {{
                "transition_type": "fade",
                "duration": 0.5,
                "parameters": {{}},
                "easing": "ease_in_out"
            }},
            "transition_out": {{
                "transition_type": "crossfade",
                "duration": 0.5,
                "parameters": {{}},
                "easing": "ease_in_out"
            }}
        }}
    ],
    "global_effects": [
        {{
            "effect_type": "color_balance",
            "parameters": {{"contrast": 1.1, "saturation": 1.05}},
            "intensity": 1.0
        }}
    ],
    "audio_settings": {{
        "normalize": true,
        "fade_in": 0.5,
        "fade_out": 0.5
    }},
    "color_settings": {{
        "contrast": 1.1,
        "saturation": 1.05,
        "brightness": 1.0
    }}
}}

RULES:
1. Use beat timestamps for cuts when available
2. Emphasize high-motion moments with appropriate effects
3. Avoid cutting during important moments
4. Create logical flow and narrative
5. Consider audio levels for timing
6. Make each edit purposeful and meaningful
7. Use MoviePy-compatible effect names and parameters
8. Ensure all timestamps are within video duration
9. Apply style-appropriate color grading and effects

Generate a JSON response only, no additional text.
"""
        return prompt
    
    def _parse_moviepy_response(
        self, 
        response: str, 
        style: EditStyle, 
        analysis: VideoAnalysisResult,
        source_video_path: str,
        output_path: str,
        target_duration: Optional[float],
        output_format: str,
        output_quality: str
    ) -> MoviePyRenderingPlan:
        """Parse LLM response into MoviePyRenderingPlan object"""
        try:
            # Parse JSON response
            data = json.loads(response)
            
            # Validate required fields
            required_fields = ["style", "target_duration", "segments", "confidence", "reasoning"]
            for field in required_fields:
                if field not in data:
                    raise ValueError(f"Missing required field: {field}")
            
            # Convert segments to MoviePySegment objects
            segments = []
            for segment_data in data.get("segments", []):
                # Convert effects
                effects = []
                for effect_data in segment_data.get("effects", []):
                    effect = MoviePyEffect(
                        effect_type=effect_data["effect_type"],
                        parameters=effect_data.get("parameters", {}),
                        start_time=effect_data.get("start_time"),
                        end_time=effect_data.get("end_time"),
                        intensity=float(effect_data.get("intensity", 1.0))
                    )
                    effects.append(effect)
                
                # Convert audio effects
                audio_effects = []
                for effect_data in segment_data.get("audio_effects", []):
                    effect = MoviePyEffect(
                        effect_type=effect_data["effect_type"],
                        parameters=effect_data.get("parameters", {}),
                        start_time=effect_data.get("start_time"),
                        end_time=effect_data.get("end_time"),
                        intensity=float(effect_data.get("intensity", 1.0))
                    )
                    audio_effects.append(effect)
                
                # Convert transitions
                transition_in = None
                if "transition_in" in segment_data and segment_data["transition_in"]:
                    trans_data = segment_data["transition_in"]
                    transition_in = MoviePyTransition(
                        transition_type=trans_data["transition_type"],
                        duration=float(trans_data.get("duration", 0.5)),
                        parameters=trans_data.get("parameters", {}),
                        easing=trans_data.get("easing", "linear")
                    )
                
                transition_out = None
                if "transition_out" in segment_data and segment_data["transition_out"]:
                    trans_data = segment_data["transition_out"]
                    transition_out = MoviePyTransition(
                        transition_type=trans_data["transition_type"],
                        duration=float(trans_data.get("duration", 0.5)),
                        parameters=trans_data.get("parameters", {}),
                        easing=trans_data.get("easing", "linear")
                    )
                
                # Create segment
                segment = MoviePySegment(
                    start_time=float(segment_data["start_time"]),
                    end_time=float(segment_data["end_time"]),
                    speed=float(segment_data.get("speed", 1.0)),
                    volume=float(segment_data.get("volume", 1.0)),
                    effects=effects,
                    audio_effects=audio_effects,
                    transition_in=transition_in,
                    transition_out=transition_out,
                    crop=segment_data.get("crop"),
                    resize=segment_data.get("resize"),
                    rotation=float(segment_data.get("rotation", 0.0)),
                    brightness=float(segment_data.get("brightness", 1.0)),
                    contrast=float(segment_data.get("contrast", 1.0)),
                    saturation=float(segment_data.get("saturation", 1.0)),
                    gamma=float(segment_data.get("gamma", 1.0)),
                    blur=float(segment_data.get("blur", 0.0)),
                    sharpness=float(segment_data.get("sharpness", 1.0))
                )
                segments.append(segment)
            
            # Convert global effects
            global_effects = []
            for effect_data in data.get("global_effects", []):
                effect = MoviePyEffect(
                    effect_type=effect_data["effect_type"],
                    parameters=effect_data.get("parameters", {}),
                    intensity=float(effect_data.get("intensity", 1.0))
                )
                global_effects.append(effect)
            
            # Create MoviePyRenderingPlan
            rendering_plan = MoviePyRenderingPlan(
                video_id=analysis.video_id,
                source_video_path=source_video_path,
                output_path=output_path,
                target_duration=float(data["target_duration"]),
                output_format=output_format,
                output_quality=output_quality,
                fps=analysis.fps,
                resolution=analysis.resolution,
                segments=segments,
                global_effects=global_effects,
                audio_settings=data.get("audio_settings", {}),
                color_settings=data.get("color_settings", {}),
                style=data["style"],
                confidence=float(data["confidence"]),
                reasoning=data["reasoning"]
            )
            
            self.logger.info(f"Successfully parsed LLM response into MoviePy rendering plan")
            return rendering_plan
            
        except (json.JSONDecodeError, KeyError, ValueError) as e:
            import traceback
            tb = traceback.format_exc()
            self.logger.error(f"âŒ [LLM EDITOR] Failed to parse MoviePy LLM response")
            self.logger.error(f"âŒ [LLM EDITOR] Exception type: {type(e).__name__}")
            self.logger.error(f"âŒ [LLM EDITOR] Exception message: {str(e)}")
            self.logger.error(f"âŒ [LLM EDITOR] Full traceback:")
            self.logger.error(tb)
            self.logger.error(f"âŒ [LLM EDITOR] Raw response:")
            self.logger.error(response)
            self.logger.error(f"âŒ [LLM EDITOR] Response length: {len(response) if response else 0}")
            self.logger.error(f"âŒ [LLM EDITOR] Response preview: {response[:500] if response else 'None'}...")
            # Return fallback plan
            return self._create_fallback_moviepy_plan(
                style, analysis, source_video_path, output_path, 
                target_duration, output_format, output_quality
            )
    
    def _create_fallback_moviepy_plan(
        self,
        style: EditStyle,
        analysis: VideoAnalysisResult,
        source_video_path: str,
        output_path: str,
        target_duration: Optional[float],
        output_format: str,
        output_quality: str
    ) -> MoviePyRenderingPlan:
        """Create a fallback MoviePy rendering plan when LLM parsing fails"""
        self.logger.warning("Creating fallback MoviePy rendering plan")
        
        # Simple fallback: split video into 3-5 segments
        duration = analysis.duration
        num_segments = min(5, max(3, int(duration / 10)))
        segment_duration = duration / num_segments
        
        segments = []
        for i in range(num_segments):
            start_time = i * segment_duration
            end_time = (i + 1) * segment_duration
            
            # Add basic effects based on style
            effects = []
            if style == EditStyle.TIKTOK:
                effects.append(MoviePyEffect(
                    effect_type="brightness",
                    parameters={"factor": 1.1},
                    intensity=1.0
                ))
            elif style == EditStyle.CINEMATIC:
                effects.append(MoviePyEffect(
                    effect_type="contrast",
                    parameters={"factor": 1.2},
                    intensity=1.0
                ))
            
            segment = MoviePySegment(
                start_time=start_time,
                end_time=end_time,
                speed=1.0,
                volume=1.0,
                effects=effects,
                audio_effects=[],
                transition_in=None,
                transition_out=None,
                brightness=1.0,
                contrast=1.0,
                saturation=1.0
            )
            segments.append(segment)
        
        return MoviePyRenderingPlan(
            video_id=analysis.video_id,
            source_video_path=source_video_path,
            output_path=output_path,
            target_duration=target_duration if target_duration else duration,
            output_format=output_format,
            output_quality=output_quality,
            fps=analysis.fps,
            resolution=analysis.resolution,
            segments=segments,
            global_effects=[],
            audio_settings={},
            color_settings={},
            style=style.value,
            confidence=0.5,
            reasoning="Fallback plan due to LLM parsing error"
        )
    
    def _create_fallback_plan(self, style: EditStyle, analysis: VideoAnalysisResult) -> EditingPlan:
        """Create a fallback editing plan when LLM parsing fails"""
        self.logger.warning("Creating fallback editing plan")
        
        # Simple fallback: split video into 3-5 segments
        duration = analysis.duration
        num_segments = min(5, max(3, int(duration / 10)))
        segment_duration = duration / num_segments
        
        segments = []
        for i in range(num_segments):
            start_time = i * segment_duration
            end_time = (i + 1) * segment_duration
            
            segment = EditingDecision(
                start_time=start_time,
                end_time=end_time,
                effect="normal",
                intensity=1.0,
                reasoning=f"Fallback segment {i+1}"
            )
            segments.append(segment)
        
        return EditingPlan(
            style=style.value,
            target_duration=duration,
            segments=segments,
            transitions=["cut"],
            effects=["normal"],
            reasoning="Fallback plan due to LLM parsing error",
            confidence=0.5
        )


def create_llm_editor(provider: str = "openai") -> LLMEditor:
    """Factory function to create LLM editor with specified provider"""
    try:
        provider_enum = LLMProvider(provider.lower())
        return LLMEditor(provider_enum)
    except ValueError:
        logger.warning(f"Unknown provider '{provider}', using openai")
        return LLMEditor(LLMProvider.OPENAI) 